{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import dypy.netcdf as dn\n",
    "import dypy.intergrid as ig\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_nr_list = []\n",
    "for i in range(1,35+1):\n",
    "    if i < 10:\n",
    "        ensemble_nr_list.append(f\"00{i}\")\n",
    "    else:\n",
    "        ensemble_nr_list.append(f\"0{i}\")\n",
    "\n",
    "years = np.arange(2091,2100+1)\n",
    "years = [str(year) for year in years]\n",
    "\n",
    "paths = [f\"/net/litho/atmosdyn/INTEXseas/cesm/cesm112_LENS/b.e112.BRCP85LENS.f09_g16.ethz.{ensemble_nr}/archive/atm/hist/b.e112.BRCP85LENS.f09_g16.ethz.{ensemble_nr}.cam.h2.{year}-01-01-21600.nc\" for ensemble_nr in ensemble_nr_list for year in years]\n",
    "\n",
    "for i,path in enumerate(paths):\n",
    "    np.array(dn.read_var(path, \"hyam\"))[0]\n",
    "    print(f\"{i//10+1}.{i%10+1} worked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths = [\"/net/litho/atmosdyn/INTEXseas/cesm/cesm112_LENS/b.e112.BRCP85LENS.f09_g16.ethz.001/archive/atm/hist/b.e112.BRCP85LENS.f09_g16.ethz.001.cam.h2.2091-01-01-21600.nc\"]\n",
    "\n",
    "lon_min_CESM = 0\n",
    "lon_max_CESM = 15\n",
    "lat_min_CESM = 42\n",
    "lat_max_CESM = 50\n",
    "\n",
    "lons, lats = dn.read_var(paths[0], [\"lon\", \"lat\"])\n",
    "\n",
    "xindex = np.where((lons >= lon_min_CESM) & (lons <= lon_max_CESM))[0]\n",
    "print(\"xindex: \" + str(xindex))\n",
    "print(\"lons: \" + str(lons[xindex]))\n",
    "yindex = np.where((lats >= lat_min_CESM) & (lats <= lat_max_CESM))[0]\n",
    "print(\"yindex: \" + str(yindex))\n",
    "print(\"lats: \" + str(lats[yindex]))\n",
    "xmin, xmax = xindex.min(), xindex.max()\n",
    "print(\"xmin: \" + str(xmin))\n",
    "print(\"xmax: \" + str(xmax))\n",
    "ymin, ymax = yindex.min(), yindex.max()\n",
    "print(\"ymin: \" + str(ymin))\n",
    "print(\"ymax: \" + str(ymax))\n",
    "\n",
    "index = np.s_[:, :, ymin:(ymax+1), xmin:(xmax+1)]\n",
    "\n",
    "lats_labels = [str(int(100*lat)) for lat in lats[yindex]]\n",
    "lons_labels = [str(int(100*lon)) for lon in lons[xindex]]\n",
    "print(lats_labels)\n",
    "print(lons_labels)\n",
    "\n",
    "lats_amount = len(lats_labels)\n",
    "lons_amount = len(lons_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CESM-f files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_list = []\n",
    "for filepath in paths:\n",
    "    print(filepath)\n",
    "\n",
    "    hyam, hybm = np.array(dn.read_var(filepath, [\"hyam\", \"hybm\"]))\n",
    "    SLP_values, PS_values =  np.array(dn.read_var(filepath, [\"PSL\", \"PS\"], index=index))\n",
    "    T_values, V_values, U_values, Z_values = np.array(dn.read_var(filepath, [\"T\", \"V\", \"U\", \"Z3\"], index=index))\n",
    "\n",
    "    for time_point in range(0,1460):\n",
    "        print(time_point)\n",
    "        feature_dict = {}\n",
    "\n",
    "        P3_t = np.tensordot(hyam, 100000*np.ones((lats_amount, lons_amount)), axes=0) + np.tensordot(hybm, PS_values[time_point], axes=0)\n",
    "\n",
    "        # Retrieve temperature at 900 hPa\n",
    "        index_lvl = np.expand_dims((P3_t < 90000).argmin(axis=0), axis=0)\n",
    "        pressure_lvl_too_low_mask = np.where(index_lvl == 0, True, False)[0]\n",
    "\n",
    "        p_2 = np.take_along_axis(P3_t, index_lvl, axis=0)[0]\n",
    "        p_1 = np.take_along_axis(P3_t, index_lvl-1, axis=0)[0]\n",
    "\n",
    "        T_2 = np.take_along_axis(T_values[time_point], index_lvl, axis=0)[0]\n",
    "        T_1 = np.take_along_axis(T_values[time_point], index_lvl-1, axis=0)[0]\n",
    "\n",
    "        delta_p = (90000-p_1)/(p_2-p_1)\n",
    "        T_interpol = (T_2-T_1)*delta_p + T_1\n",
    "\n",
    "        T_interpol[pressure_lvl_too_low_mask] = np.NaN\n",
    "\n",
    "        feature_names = [f\"T_{lat}_{lon}_900\" for lat in lats_labels for lon in lons_labels]\n",
    "        feature_dict.update(zip(feature_names, T_interpol.flatten()))\n",
    "\n",
    "\n",
    "        for p in [50000, 70000, 85000]:\n",
    "\n",
    "            index_lvl = np.expand_dims((P3_t < p).argmin(axis=0), axis=0)\n",
    "            pressure_lvl_too_low_mask = np.where(index_lvl == 0, True, False)[0]\n",
    "\n",
    "            p_2 = np.take_along_axis(P3_t, index_lvl, axis=0)[0]\n",
    "            p_1 = np.take_along_axis(P3_t, index_lvl-1, axis=0)[0]\n",
    "\n",
    "            T_2 = np.take_along_axis(T_values[time_point], index_lvl, axis=0)[0]\n",
    "            T_1 = np.take_along_axis(T_values[time_point], index_lvl-1, axis=0)[0]\n",
    "\n",
    "            V_2 = np.take_along_axis(V_values[time_point], index_lvl, axis=0)[0]\n",
    "            V_1 = np.take_along_axis(V_values[time_point], index_lvl-1, axis=0)[0]\n",
    "\n",
    "            Z_2 = np.take_along_axis(Z_values[time_point], index_lvl, axis=0)[0]\n",
    "            Z_1 = np.take_along_axis(Z_values[time_point], index_lvl-1, axis=0)[0]\n",
    "\n",
    "            U_2 = np.take_along_axis(U_values[time_point], index_lvl, axis=0)[0]\n",
    "            U_1 = np.take_along_axis(U_values[time_point], index_lvl-1, axis=0)[0]\n",
    "\n",
    "            delta_p = (p-p_1)/(p_2-p_1)\n",
    "            T_interpol = (T_2-T_1)*delta_p + T_1\n",
    "            V_interpol = (V_2-V_1)*delta_p + V_1\n",
    "            Z_interpol = (Z_2-Z_1)*delta_p + Z_1\n",
    "            U_interpol = (U_2-U_1)*delta_p + U_1\n",
    "\n",
    "            T_interpol[pressure_lvl_too_low_mask] = np.NaN\n",
    "            V_interpol[pressure_lvl_too_low_mask] = np.NaN\n",
    "            Z_interpol[pressure_lvl_too_low_mask] = np.NaN\n",
    "            U_interpol[pressure_lvl_too_low_mask] = np.NaN\n",
    "\n",
    "            feature_names = [f\"T_{lat}_{lon}_{p//100}\" for lat in lats_labels for lon in lons_labels]\n",
    "            feature_dict.update(zip(feature_names, T_interpol.flatten()))\n",
    "\n",
    "            feature_names = [f\"V_{lat}_{lon}_{p//100}\" for lat in lats_labels for lon in lons_labels]\n",
    "            feature_dict.update(zip(feature_names, V_interpol.flatten()))\n",
    "\n",
    "            feature_names = [f\"Z_{lat}_{lon}_{p//100}\" for lat in lats_labels for lon in lons_labels]\n",
    "            feature_dict.update(zip(feature_names, Z_interpol.flatten()))\n",
    "\n",
    "            feature_names = [f\"U_{lat}_{lon}_{p//100}\" for lat in lats_labels for lon in lons_labels]\n",
    "            feature_dict.update(zip(feature_names, U_interpol.flatten()))\n",
    "\n",
    "        SLP_values_t = SLP_values[time_point]/100\n",
    "\n",
    "        feature_names = [f\"SLP_{lat}_{lon}_sealevel\" for lat in lats_labels for lon in lons_labels]\n",
    "        feature_dict.update(zip(feature_names, SLP_values_t.flatten()))\n",
    "\n",
    "        rows_list.append(feature_dict)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows_list)\n",
    "df.dropna(axis=1, inplace = True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = np.arange(1,12+1)\n",
    "months = [str(month) for month in months]\n",
    "\n",
    "days_in_month = {\"01\": 31,\n",
    "                 \"02\": 28,\n",
    "                 \"03\": 31,\n",
    "                 \"04\": 30,\n",
    "                 \"05\": 31,\n",
    "                 \"06\": 30,\n",
    "                 \"07\": 31,\n",
    "                 \"08\": 31,\n",
    "                 \"09\": 30,\n",
    "                 \"10\": 31,\n",
    "                 \"11\": 30,\n",
    "                 \"12\": 31,\n",
    "                }\n",
    "\n",
    "hours =[\"00\", \"06\", \"12\", \"18\"]\n",
    "\n",
    "for i in range(0,9):\n",
    "    months[i] = \"0\" + months[i]\n",
    "\n",
    "dates = [year + \"-\" + month + \"-\" + str(day) + \" \" + hour+ \":00\" for year in years for month in months for day in range(1, days_in_month[month]+1) for hour in hours]\n",
    "\n",
    "ensembles = [f\"E{nr}\" for nr in range(1,35+1) for i in range(len(dates))]\n",
    "print(len(ensembles))\n",
    "print(len(dates))\n",
    "\n",
    "dates = dates*35\n",
    "print(len(dates))\n",
    "\n",
    "df_dates = pd.DataFrame(dates, columns=[\"date\"])\n",
    "df_ensembles = pd.DataFrame(ensembles, columns=[\"ensemble\"])\n",
    "df['date'] = pd.to_datetime(df_dates['date'], format=\"%Y-%m-%d %H:%M\")\n",
    "df[\"ensemble\"] = df_ensembles[\"ensemble\"]\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write recipe outputs\n",
    "df.to_csv(\"CESMf_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
