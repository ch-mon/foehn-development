{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import dypy.netcdf as dn\n",
    "import dypy.intergrid as ig\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W-E direction\n",
    "LON_MIN_ERA= 0\n",
    "LON_MAX_ERA = 15\n",
    "\n",
    "# S-N direction\n",
    "LAT_MIN_ERA= 42\n",
    "LAT_MAX_ERA = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(1981,2019+1)\n",
    "years = [str(year) for year in years]\n",
    "\n",
    "months = np.arange(1,12+1)\n",
    "months = [str(month) for month in months]\n",
    "\n",
    "days = np.arange(1,31+1)\n",
    "days = [str(day) for day in days]\n",
    "\n",
    "hours =[\"00\", \"06\", \"12\", \"18\"]\n",
    "\n",
    "for i in range(0,9):\n",
    "    months[i] = \"0\" + months[i]\n",
    "    days[i] = \"0\" + days[i]\n",
    "\n",
    "print(years)\n",
    "print(months)\n",
    "print(days)\n",
    "print(hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/net/litho/atmosdyn/INTEXseas/cesm/cesm112_LENS/b.e112.B20TRLENS.f09_g16.ethz.001/archive/atm/hist/b.e112.B20TRLENS.f09_g16.ethz.001.cam.h2.1990-01-01-21600.nc\"\n",
    "\n",
    "lons, lats = dn.read_var(path, [\"lon\", \"lat\"])\n",
    "\n",
    "xindex = np.where((lons >= LON_MIN_ERA) & (lons <= LON_MAX_ERA))[0]\n",
    "print(\"xindex: \" + str(xindex))\n",
    "print(\"lons: \" + str(lons[xindex]))\n",
    "yindex = np.where((lats >= LAT_MIN_ERA) & (lats <= LAT_MAX_ERA))[0]\n",
    "print(\"yindex: \" + str(yindex))\n",
    "print(\"lats: \" + str(lats[yindex]))\n",
    "xmin, xmax = xindex.min(), xindex.max()\n",
    "print(\"xmin: \" + str(xmin))\n",
    "print(\"xmax: \" + str(xmax))\n",
    "ymin, ymax = yindex.min(), yindex.max()\n",
    "print(\"ymin: \" + str(ymin))\n",
    "print(\"ymax: \" + str(ymax))\n",
    "\n",
    "index = np.s_[:, :, ymin:(ymax+1), xmin:(xmax+1)]\n",
    "\n",
    "lo = np.array([LAT_MIN_ERA, LON_MIN_ERA])\n",
    "hi = np.array([LAT_MAX_ERA, LON_MAX_ERA])\n",
    "\n",
    "query_points = [[lat, lon] for lat in lats[yindex] for lon in lons[xindex]]\n",
    "query_points_labels = [[str(int(100*query_point[0])), str(int(100*query_point[1]))] for query_point in query_points]\n",
    "print(query_points_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(grid, lats, lons, annot_bool=False):\n",
    "    df = pd.DataFrame(grid, index=lats, columns=lons)\n",
    "\n",
    "    fig = plt.figure(figsize=(15,9))\n",
    "    sns.heatmap(df, annot=annot_bool)\n",
    "\n",
    "    # Due to bug in matplotlib\n",
    "    b, t = plt.ylim()\n",
    "    b += 0.5\n",
    "    t -= 0.5\n",
    "    plt.ylim(b, t)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in all relevant ERAI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootpath = \"/net/bio/atmosdyn/erainterim/cdf/\"\n",
    "\n",
    "file_letters = [\"P\", \"R\", \"Z\"]\n",
    "\n",
    "rows_list = []\n",
    "for year in years:\n",
    "    print(\"Year: \" + year)\n",
    "    yearpath = rootpath + year + \"/\"\n",
    "\n",
    "    for month in months:\n",
    "        monthpath = yearpath + month + \"/\"\n",
    "\n",
    "        for day in days:\n",
    "\n",
    "            for hour in hours:\n",
    "\n",
    "                feature_dict = {\"date\": year + \"-\" + month +\"-\" + day + \" \" + hour + \":00\"}\n",
    "\n",
    "                for letter in file_letters:\n",
    "                    filepath = monthpath + letter + year + month + day + \"_\" + hour\n",
    "\n",
    "                    if letter == \"P\":\n",
    "                        try:\n",
    "                            SLP, = dn.read_var(filepath, ['SLP'])\n",
    "                        except:\n",
    "                            print(\"Couldn't read file: \" + letter + year + month + day +\"_\" +hour)\n",
    "                            feature_dict.clear()\n",
    "                            break\n",
    "\n",
    "                        SLP_cut = SLP[(LAT_MIN_ERA+90):(LAT_MAX_ERA+90+1),(180+LON_MIN_ERA):(180+LON_MAX_ERA+1)]\n",
    "\n",
    "                        interfunc_SLP = ig.Intergrid(SLP_cut, lo=lo, hi=hi, verbose = False)\n",
    "                        SLP_query_values = interfunc_SLP(query_points)\n",
    "\n",
    "                        feature_names = [\"SLP_\" + query_point[0] + \"_\" + query_point[1] + \"_sealevel\" for query_point in query_points_labels]\n",
    "                        feature_dict.update(zip(feature_names, SLP_query_values))\n",
    "\n",
    "\n",
    "                    if letter == \"Z\":\n",
    "                        try:\n",
    "                            Z,T,Q,U,V = dn.read_var(filepath, ['Z','T','Q','U','V'])\n",
    "                        except:\n",
    "                            print(\"Couldn't read file: \" + letter + year + month + day +\"_\" +hour)\n",
    "                            feature_dict.clear()\n",
    "                            break\n",
    "\n",
    "                        T_cut = T[0][(LAT_MIN_ERA+90):(LAT_MAX_ERA+90+1),(180+LON_MIN_ERA):(180+LON_MAX_ERA+1)]\n",
    "                        interfunc_T = ig.Intergrid(T_cut, lo=lo, hi=hi, verbose = False)\n",
    "                        T_query_values = interfunc_T(query_points)\n",
    "                        feature_names = [\"T_\" + query_point[0] + \"_\" + query_point[1] + \"_900\" for query_point in query_points_labels]\n",
    "                        feature_dict.update(zip(feature_names, T_query_values))\n",
    "\n",
    "\n",
    "                        for index, pressure_level in {1: \"850\", 3: \"700\", 5: \"500\"}.items():\n",
    "\n",
    "                            Z_cut = Z[index][(LAT_MIN_ERA+90):(LAT_MAX_ERA+90+1),(180+LON_MIN_ERA):(180+LON_MAX_ERA+1)]\n",
    "                            T_cut = T[index][(LAT_MIN_ERA+90):(LAT_MAX_ERA+90+1),(180+LON_MIN_ERA):(180+LON_MAX_ERA+1)]\n",
    "                            Q_cut = Q[index][(LAT_MIN_ERA+90):(LAT_MAX_ERA+90+1),(180+LON_MIN_ERA):(180+LON_MAX_ERA+1)]\n",
    "                            U_cut = U[index][(LAT_MIN_ERA+90):(LAT_MAX_ERA+90+1),(180+LON_MIN_ERA):(180+LON_MAX_ERA+1)]\n",
    "                            V_cut = V[index][(LAT_MIN_ERA+90):(LAT_MAX_ERA+90+1),(180+LON_MIN_ERA):(180+LON_MAX_ERA+1)]\n",
    "\n",
    "                            interfunc_Z = ig.Intergrid(Z_cut, lo=lo, hi=hi, verbose = False)\n",
    "                            interfunc_T = ig.Intergrid(T_cut, lo=lo, hi=hi, verbose = False)\n",
    "                            interfunc_Q = ig.Intergrid(Q_cut, lo=lo, hi=hi, verbose = False)\n",
    "                            interfunc_U = ig.Intergrid(U_cut, lo=lo, hi=hi, verbose = False)\n",
    "                            interfunc_V = ig.Intergrid(V_cut, lo=lo, hi=hi, verbose = False)\n",
    "\n",
    "                            Z_query_values = interfunc_Z(query_points)\n",
    "                            T_query_values = interfunc_T(query_points)\n",
    "                            Q_query_values = interfunc_Q(query_points)\n",
    "                            U_query_values = interfunc_U(query_points)\n",
    "                            V_query_values = interfunc_V(query_points)\n",
    "\n",
    "                            feature_names = [\"Z_\" + query_point[0] + \"_\" + query_point[1] + \"_\" + pressure_level for query_point in query_points_labels]\n",
    "                            feature_dict.update(zip(feature_names, Z_query_values))\n",
    "\n",
    "                            feature_names = [\"T_\" + query_point[0] + \"_\" + query_point[1] + \"_\" + pressure_level for query_point in query_points_labels]\n",
    "                            feature_dict.update(zip(feature_names, T_query_values))\n",
    "\n",
    "                            feature_names = [\"Q_\" + query_point[0] + \"_\" + query_point[1] + \"_\" + pressure_level for query_point in query_points_labels]\n",
    "                            feature_dict.update(zip(feature_names, Q_query_values))\n",
    "\n",
    "                            feature_names = [\"U_\" + query_point[0] + \"_\" + query_point[1] + \"_\" + pressure_level for query_point in query_points_labels]\n",
    "                            feature_dict.update(zip(feature_names, U_query_values))\n",
    "\n",
    "                            feature_names = [\"V_\" + query_point[0] + \"_\" + query_point[1] + \"_\" + pressure_level for query_point in query_points_labels]\n",
    "                            feature_dict.update(zip(feature_names, V_query_values))\n",
    "\n",
    "\n",
    "                rows_list.append(feature_dict)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows_list)\n",
    "df.dropna(inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], format='%Y%m%d %H:%M')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write recipe outputs\n",
    "df.to_csv(\"ERAI_on_CESM_grid_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
