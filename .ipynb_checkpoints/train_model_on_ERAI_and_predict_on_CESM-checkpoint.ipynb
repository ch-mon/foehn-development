{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import scipy.stats as sci\n",
    "import sklearn\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"deep\")\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import catboost as catb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from plot_mean_foehn_conditions import calculate_stability, plot_mean_foehn_condition_for_one_model, generate_coordinates_from_feature_label\n",
    "from utils import calc_pot_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATS_CESM_STRING = ['4287', '4382', '4476', '4570', '4664', '4759', '4853', '4947']\n",
    "LONS_CESM_STRING = ['0', '125', '250', '375', '500', '625', '750', '875', '1000', '1125', '1250', '1375', '1500']\n",
    "\n",
    "MONTH_NAMES = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dez\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read ERA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ERA_raw = pd.read_csv(\"data/MeteorologicalData/ERAI_data.csv.gz\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ERA = df_ERA_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CESM data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CESMp = pd.read_csv(\"data/MeteorologicalData/CESM_present_day.csv.gz\")\n",
    "df_CESMp[\"date\"] = pd.to_datetime(df_CESMp[\"date\"], format=\"%Y-%m-%dT%H:%M:00.000000Z\") # Due to Dataiku date format\n",
    "CESMp_features = set(df_CESMp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CESMf = pd.read_csv(\"data/MeteorologicalData/CESM_future_day.csv.gz\")\n",
    "df_CESMf[\"date\"] = pd.to_datetime(df_CESMf[\"date\"], format=\"%Y-%m-%dT%H:%M:00.000000Z\") # Due to Dataiku date format\n",
    "CESMf_features = set(df_CESMf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressure lvl doesnt exist (700-900 hPa, 4287, 0)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4287, 0)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4287, 125)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4287, 125)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4382, 250)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4382, 250)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4382, 375)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4382, 375)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4382, 500)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4382, 500)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4476, 250)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4476, 250)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4476, 375)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4476, 375)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4476, 500)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4476, 500)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4476, 625)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4476, 625)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4476, 750)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4476, 750)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4476, 875)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4476, 875)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4570, 375)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4570, 375)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4570, 500)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4570, 500)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4570, 625)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4570, 625)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4570, 750)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4570, 750)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4570, 875)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4570, 875)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4570, 1000)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4570, 1000)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4570, 1125)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4570, 1125)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4570, 1250)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4570, 1250)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4570, 1375)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4570, 1375)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4664, 625)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4664, 625)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4664, 750)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4664, 750)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4664, 875)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4664, 875)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4664, 1000)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4664, 1000)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4664, 1125)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4664, 1125)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4664, 1250)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4664, 1250)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4664, 1375)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4664, 1375)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4664, 1500)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4664, 1500)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4759, 750)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4759, 750)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4759, 875)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4759, 875)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4759, 1000)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4759, 1000)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4759, 1125)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4759, 1125)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4759, 1250)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4759, 1250)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4759, 1375)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4759, 1375)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4759, 1500)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4759, 1500)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4853, 1125)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4853, 1125)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4853, 1250)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4853, 1250)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4853, 1375)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4853, 1375)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4853, 1500)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4853, 1500)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4947, 1250)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4947, 1250)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4947, 1375)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4947, 1375)\n",
      "Pressure lvl doesnt exist (700-900 hPa, 4947, 1500)\n",
      "Pressure lvl doesnt exist (850-900 hPa, 4947, 1500)\n"
     ]
    }
   ],
   "source": [
    "# Calculate and append the stability parameters\n",
    "df_CESMp = calculate_stability(df = df_CESMp, lats = LATS_CESM_STRING, lons = LONS_CESM_STRING)\n",
    "df_CESMf = calculate_stability(df = df_CESMf, lats = LATS_CESM_STRING, lons = LONS_CESM_STRING)\n",
    "\n",
    "# Create pot. temp features for CESMp\n",
    "df_T= df_CESMp.filter(regex=(\"T\\w+900\")).add_prefix(\"PHI\")\n",
    "df_PHIT_900 = calc_pot_temp(T=df_T, p = 900.0)\n",
    "df_T= df_CESMp.filter(regex=(\"T\\w+850\")).add_prefix(\"PHI\")\n",
    "df_PHIT_850 = calc_pot_temp(T=df_T, p = 850.0)\n",
    "df_T= df_CESMp.filter(regex=(\"T\\w+700\")).add_prefix(\"PHI\")\n",
    "df_PHIT_700 = calc_pot_temp(T=df_T, p = 700.0)\n",
    "df_CESMp = pd.concat([df_CESMp, df_PHIT_900, df_PHIT_850, df_PHIT_700], axis=1)\n",
    "\n",
    "# Create pot. temp features for CESMf\n",
    "df_T= df_CESMf.filter(regex=(\"T\\w+900\")).add_prefix(\"PHI\")\n",
    "df_PHIT_900 = calc_pot_temp(T=df_T, p = 900.0)\n",
    "df_T= df_CESMf.filter(regex=(\"T\\w+850\")).add_prefix(\"PHI\")\n",
    "df_PHIT_850 = calc_pot_temp(T=df_T, p = 850.0)\n",
    "df_T= df_CESMf.filter(regex=(\"T\\w+700\")).add_prefix(\"PHI\")\n",
    "df_PHIT_700 = calc_pot_temp(T=df_T, p = 700.0)\n",
    "df_CESMf = pd.concat([df_CESMf, df_PHIT_900, df_PHIT_850, df_PHIT_700], axis=1)\n",
    "\n",
    "del df_PHIT_900, df_PHIT_850, df_PHIT_700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use only features which are also in CESM and which are not in the Alps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only allow features which also exist in CESM\n",
    "intersecting_features = list(CESMp_features.intersection(CESMf_features).intersection(set(df_ERA.columns)))\n",
    "intersecting_features.append(\"Foehn\")\n",
    "df_ERA = df_ERA[intersecting_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut Alps as an rectangle and drop those features\n",
    "lat_features_to_cut = set(df_ERA.loc[0:1,:].filter(regex=(\"\\w+(4570|4664|4759)\\w+\")).columns)\n",
    "lon_features_to_cut = set(df_ERA.loc[0:1,:].filter(regex=(\"\\w+(500|625|750|875|1000|1125|1250)\\w+\")).columns)\n",
    "\n",
    "intersecting_features = lat_features_to_cut.intersection(lon_features_to_cut)\n",
    "\n",
    "df_ERA.drop(intersecting_features, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ERA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate derived variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sealevel pressure differences\n",
    "df_pressures = df_ERA.filter(regex=(\"SLP_\\w+\"))\n",
    "\n",
    "pressure_list = sorted(df_pressures.columns.tolist())\n",
    "pressure_list_dummy = sorted(df_pressures.columns.tolist())\n",
    "\n",
    "SLP_dict = {}\n",
    "for col1 in pressure_list:\n",
    "    pressure_list_dummy.remove(col1)\n",
    "    for col2 in pressure_list_dummy:\n",
    "        SLP_dict[f\"diff_{col1}_{col2}\"] = (df_pressures.loc[:, col1] - df_pressures.loc[:, col2]).values\n",
    "\n",
    "#df_pressures = pd.concat([df_pressures, pd.DataFrame(SLP_dict)], axis = 1)\n",
    "df_pressures = pd.DataFrame(SLP_dict)\n",
    "\n",
    "del SLP_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geopotential height (on 850 and 700 hPa)\n",
    "\n",
    "df_Z = df_ERA.filter(regex=(\"Z_\\w+\"))\n",
    "Z_dict = {}\n",
    "\n",
    "for level in [\"850\", \"700\"]:\n",
    "    df_Z_level = df_Z.filter(regex=(\"Z_\\w+\" + level))\n",
    "\n",
    "    Z_list = sorted(df_Z_level.columns.tolist())\n",
    "    Z_list_dummy = sorted(df_Z_level.columns.tolist())\n",
    "\n",
    "    for col1 in Z_list:\n",
    "        Z_list_dummy.remove(col1)\n",
    "        for col2 in Z_list_dummy:\n",
    "            Z_dict[f\"diff_{col1}_{col2}\"] = (df_Z_level.loc[:, col1] - df_Z_level.loc[:, col2]).values\n",
    "\n",
    "#df_Z = pd.concat([df_Z, pd.DataFrame(Z_dict)], axis = 1)\n",
    "df_Z = pd.DataFrame(Z_dict)\n",
    "\n",
    "del df_Z_level, Z_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical potential temperature differences (on 850 hPa)\n",
    "\n",
    "df_T = df_ERA.filter(regex=(\"T_\\w+850\"))\n",
    "df_PHIT = calc_pot_temp(T=df_T+273.15, p=850.0).add_prefix(\"PHI\")\n",
    "PHIT_dict = {}\n",
    "\n",
    "PHIT_list = sorted(df_PHIT.columns.tolist())\n",
    "PHIT_list_dummy = sorted(df_PHIT.columns.tolist())\n",
    "\n",
    "for col1 in PHIT_list:\n",
    "    PHIT_list_dummy.remove(col1)\n",
    "    for col2 in PHIT_list_dummy:\n",
    "        PHIT_dict[f\"diff_{col1}_{col2}\"] = (df_PHIT.loc[:, col1] - df_PHIT.loc[:, col2]).values\n",
    "\n",
    "#df_PHIT = pd.concat([df_PHIT, pd.DataFrame(PHIT_dict)], axis = 1)\n",
    "df_PHIT = pd.DataFrame(PHIT_dict)\n",
    "\n",
    "del df_T, PHIT_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stability variables\n",
    "\n",
    "df_T= df_ERA.filter(regex=(\"T\\w+900\")).add_prefix(\"PHI\")\n",
    "df_PHIT_900 = calc_pot_temp(T=df_T+273.15, p = 900.0)\n",
    "df_T= df_ERA.filter(regex=(\"T\\w+850\")).add_prefix(\"PHI\")\n",
    "df_PHIT_850 = calc_pot_temp(T=df_T+273.15, p = 850.0)\n",
    "df_T= df_ERA.filter(regex=(\"T\\w+700\")).add_prefix(\"PHI\")\n",
    "df_PHIT_700 = calc_pot_temp(T=df_T+273.15, p = 700.0)\n",
    "\n",
    "\n",
    "stability_dict = {}\n",
    "for lat in LATS_CESM_STRING:\n",
    "    for lon in LONS_CESM_STRING:\n",
    "        try:\n",
    "            stability_dict[f\"DELTAPHI_{lat}_{lon}_850\"] = (df_PHIT_850.loc[:, f\"PHIT_{lat}_{lon}_850\"] - df_PHIT_900.loc[:, f\"PHIT_{lat}_{lon}_900\"]).values\n",
    "            stability_dict[f\"DELTAPHI_{lat}_{lon}_700\"] = (df_PHIT_700.loc[:, f\"PHIT_{lat}_{lon}_700\"] - df_PHIT_900.loc[:, f\"PHIT_{lat}_{lon}_900\"]).values\n",
    "        except:\n",
    "            print(\"Variable does not exist at this location.\")\n",
    "df_stability = pd.DataFrame(stability_dict)\n",
    "\n",
    "del df_T, df_PHIT_900, df_PHIT_850, df_PHIT_700, stability_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind velocities\n",
    "df_wind = df_ERA.filter(regex=(\"(U|V)\\w+\")).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ERA = pd.concat([df_ERA[\"date\"], \n",
    "                     df_pressures, \n",
    "                     df_Z, \n",
    "                     df_PHIT, \n",
    "                     df_stability, \n",
    "                     df_wind], \n",
    "                    axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge in foehn data at specified location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location=\"LUG\"\n",
    "df_foehn = pd.read_csv(f\"data/FoehnData/{location}_foehn.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Merge ERAI and foehn data\n",
    "df_ERA_all_features = pd.merge(df_ERA, df_foehn, on=\"date\", how=\"left\", validate=\"one_to_one\")\n",
    "\n",
    "# Drop rows with missing foeh  values\n",
    "df_ERA_all_features.dropna(inplace=True, how=\"any\", axis=0)\n",
    "df_ERA_all_features.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test mask\n",
    "test_mask = (df_ERA_all_features[\"date\"]>np.datetime64(\"1991-01-01 00:00\")) & (df_ERA_all_features[\"date\"]<np.datetime64(\"2000-12-31 23:00\"))\n",
    "train_mask = (test_mask == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of all features for prediction\n",
    "feature_names=df_ERA_all_features.columns.tolist()\n",
    "feature_names.remove(\"date\")\n",
    "feature_names.remove(\"Foehn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_clf_on_all_features(model, df_ERA, feature_names):\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(df_ERA.loc[train_mask, feature_names], \n",
    "              df_ERA.loc[train_mask, \"Foehn\"])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict_proba(df_ERA.loc[test_mask, feature_names])\n",
    "    \n",
    "    # Calculate best threshold\n",
    "    precisions, recalls, thresholds = precision_recall_curve(df_ERA.loc[test_mask, \"Foehn\"], predictions[:,1])\n",
    "    best_threshold = thresholds[np.argmin(abs(precisions-recalls))]\n",
    "    predictions = (predictions[:,1]>best_threshold).astype(int)\n",
    "    \n",
    "    # Plot precision and recall curves\n",
    "    f = plt.figure(figsize=(12,5))\n",
    "    f.add_subplot(121)\n",
    "    sns.lineplot(precisions, recalls)\n",
    "    f.add_subplot(122)\n",
    "    sns.lineplot(np.append(thresholds, 1.0), precisions)\n",
    "    sns.lineplot(np.append(thresholds, 1.0), recalls)\n",
    "    \n",
    "    # Print best threshold, precision, recall and confusion matrix\n",
    "    print(f\"Best threshold: {best_threshold}\")\n",
    "    print(f'Precision: {precision_score(df_ERA.loc[test_mask, \"Foehn\"], predictions)}')\n",
    "    print(f'Recall: {recall_score(df_ERA.loc[test_mask, \"Foehn\"], predictions)}')\n",
    "    print(confusion_matrix(df_ERA.loc[test_mask, \"Foehn\"], predictions))\n",
    "    \n",
    "    # Show 50 most important features\n",
    "    df_ERA_feature_importances = pd.DataFrame({\"feature_name\": feature_names, \"importance\": model.feature_importances_}).sort_values(by=\"importance\", ascending=False).reset_index(drop=True)\n",
    "    display(df_ERA_feature_importances.head(50))\n",
    "    \n",
    "    return df_ERA_feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cat = catb.CatBoostClassifier(loss_function='Logloss',\n",
    "                                verbose=True,\n",
    "                                scale_pos_weight=20,\n",
    "                                thread_count=20,\n",
    "                                max_depth=4, \n",
    "                                learning_rate=0.1, \n",
    "                                n_estimators=1000,\n",
    "                                reg_lambda=10,\n",
    "                                   random_state=42)\n",
    "\n",
    "df_ERA_feature_importances = fit_clf_on_all_features(model_cat, df_ERA_all_features, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a global classifier on all features\n",
    "model_xg = xgb.XGBClassifier(\n",
    "                    max_depth=4,\n",
    "                    learning_rate=0.1,\n",
    "                    gamma=0.0,\n",
    "                    min_child_weight=0.0,\n",
    "                    max_delta_step=0.0,\n",
    "                    subsample=1.0,\n",
    "                    colsample_bytree=1.0,\n",
    "                    colsample_bylevel=1.0,\n",
    "                    reg_alpha=100.0,\n",
    "                    reg_lambda=1.0,\n",
    "                    n_estimators=200,\n",
    "                    verbosity=2,\n",
    "                    nthread=20,\n",
    "                    scale_pos_weight=20.0,\n",
    "                    base_score=0.2,\n",
    "                    seed=1337,\n",
    "                    missing=None,\n",
    "                  )\n",
    "df_ERA_feature_importances = fit_clf_on_all_features(model_xg, df_ERA_all_features, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce dataframe to most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ERA_reduced_features = df_ERA_all_features.loc[:,df_ERA_feature_importances.loc[:100, \"feature_name\"]]\n",
    "feature_names_reduced = df_ERA_reduced_features.columns.tolist()\n",
    "df_ERA_reduced_features[\"date\"] = df_ERA_all_features[\"date\"]\n",
    "df_ERA_reduced_features[\"Foehn\"] = df_ERA_all_features[\"Foehn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform features to quantiles on ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_scaler_ERA = QuantileTransformer(subsample=10**(6), random_state=42)\n",
    "quantile_scaler_ERA.fit(df_ERA_reduced_features.loc[train_mask, feature_names_reduced]) # Only fit on training data\n",
    "\n",
    "df_ERA_reduced_features_scaled = pd.DataFrame(quantile_scaler_ERA.transform(df_ERA_reduced_features.loc[:, feature_names_reduced]), \n",
    "                                              columns = feature_names_reduced, \n",
    "                                              index=df_ERA_reduced_features.index)\n",
    "df_ERA_reduced_features_scaled[\"date\"] = df_ERA_all_features[\"date\"]\n",
    "df_ERA_reduced_features_scaled[\"Foehn\"] = df_ERA_all_features[\"Foehn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and preprocess CESM features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features which are used by the simplified model\n",
    "feature_dict_CESMp = {}\n",
    "feature_dict_CESMf = {}\n",
    "for feature_name in feature_names_reduced:\n",
    "    if feature_name[0:6] == \"diff_S\":\n",
    "        feature_name_splitted = feature_name.split(\"_\")\n",
    "        first_feature = \"_\".join(feature_name_splitted[1:5])\n",
    "        second_feature = \"_\".join(feature_name_splitted[5:9])\n",
    "        \n",
    "        feature_dict_CESMp[f\"diff_{first_feature}_{second_feature}\"] = (df_CESMp.loc[:, first_feature] - df_CESMp.loc[:, second_feature]).values\n",
    "        feature_dict_CESMf[f\"diff_{first_feature}_{second_feature}\"] = (df_CESMf.loc[:, first_feature] - df_CESMf.loc[:, second_feature]).values\n",
    "\n",
    "    elif feature_name[0:6] == \"diff_Z\":\n",
    "        feature_name_splitted = feature_name.split(\"_\")\n",
    "        first_feature = \"_\".join(feature_name_splitted[1:5])\n",
    "        second_feature = \"_\".join(feature_name_splitted[5:9])\n",
    "        \n",
    "        feature_dict_CESMp[f\"diff_{first_feature}_{second_feature}\"] = (df_CESMp.loc[:, first_feature] - df_CESMp.loc[:, second_feature]).values\n",
    "        feature_dict_CESMf[f\"diff_{first_feature}_{second_feature}\"] = (df_CESMf.loc[:, first_feature] - df_CESMf.loc[:, second_feature]).values\n",
    "\n",
    "    elif feature_name[0:6] == \"diff_P\":\n",
    "        feature_name_splitted = feature_name.split(\"_\")\n",
    "        first_feature = \"_\".join(feature_name_splitted[1:5])\n",
    "        second_feature = \"_\".join(feature_name_splitted[5:9])\n",
    "        \n",
    "        feature_dict_CESMp[f\"diff_{first_feature}_{second_feature}\"] = (df_CESMp.loc[:, first_feature] - df_CESMp.loc[:, second_feature]).values\n",
    "        feature_dict_CESMf[f\"diff_{first_feature}_{second_feature}\"] = (df_CESMf.loc[:, first_feature] - df_CESMf.loc[:, second_feature]).values\n",
    "\n",
    "    elif feature_name[0:6] == \"DELTAP\":\n",
    "        feature_name_splitted = feature_name.split(\"_\")\n",
    "        first_feature = \"PHIT_\" + \"_\".join(feature_name_splitted[1:4])\n",
    "        second_feature = \"PHIT_\" + \"_\".join(feature_name_splitted[1:3]) + \"_900\"\n",
    "        \n",
    "        feature_dict_CESMp[\"DELTAPHI_\" +\"_\".join(feature_name_splitted[1:4])] = (df_CESMp.loc[:, first_feature] - df_CESMp.loc[:, second_feature]).values\n",
    "        feature_dict_CESMf[\"DELTAPHI_\" +\"_\".join(feature_name_splitted[1:4])] = (df_CESMf.loc[:, first_feature] - df_CESMf.loc[:, second_feature]).values\n",
    "    \n",
    "    else:\n",
    "        feature_dict_CESMp[feature_name] = df_CESMp.loc[:, feature_name].values\n",
    "        feature_dict_CESMf[feature_name] = df_CESMf.loc[:, feature_name].values\n",
    "\n",
    "feature_dict_CESMp[\"date\"] =df_CESMp.loc[:, \"date\"].values\n",
    "feature_dict_CESMf[\"date\"] =df_CESMf.loc[:, \"date\"].values\n",
    "\n",
    "feature_dict_CESMp[\"ensemble\"] =df_CESMp.loc[:, \"ensemble\"].values\n",
    "feature_dict_CESMf[\"ensemble\"] =df_CESMf.loc[:, \"ensemble\"].values\n",
    "\n",
    "df_CESMp_reduced_features = pd.DataFrame(feature_dict_CESMp)\n",
    "df_CESMf_reduced_features = pd.DataFrame(feature_dict_CESMf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_scaler_CESM = QuantileTransformer(subsample=10**(6), random_state=42)\n",
    "quantile_scaler_CESM.fit(df_CESMp_reduced_features.loc[:, feature_names_reduced])\n",
    "\n",
    "df_CESMp_reduced_features_scaled = pd.DataFrame(quantile_scaler_CESM.transform(df_CESMp_reduced_features.loc[:, feature_names_reduced]), \n",
    "                                                columns = feature_names_reduced, \n",
    "                                                index=df_CESMp_reduced_features.loc[:, feature_names_reduced].index)\n",
    "\n",
    "df_CESMf_reduced_features_scaled = pd.DataFrame(quantile_scaler_CESM.transform(df_CESMf_reduced_features.loc[:, feature_names_reduced]), \n",
    "                                                columns = feature_names_reduced, \n",
    "                                                index=df_CESMf_reduced_features.loc[:, feature_names_reduced].index)\n",
    "\n",
    "df_CESMp_reduced_features_scaled[\"date\"] = df_CESMp_reduced_features[\"date\"]\n",
    "df_CESMf_reduced_features_scaled[\"date\"] = df_CESMf_reduced_features[\"date\"]\n",
    "\n",
    "df_CESMp_reduced_features_scaled[\"ensemble\"] = df_CESMp_reduced_features[\"ensemble\"]\n",
    "df_CESMf_reduced_features_scaled[\"ensemble\"] = df_CESMf_reduced_features[\"ensemble\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit XGBoost to reduced feature set & evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_by_month(precision_scores, recall_scores, f1_scores):\n",
    "    plt.figure(figsize=(16,9))\n",
    "    plt.plot(MONTH_NAMES, precision_scores)\n",
    "    plt.plot(MONTH_NAMES, recall_scores)\n",
    "    plt.plot(MONTH_NAMES, f1_scores)\n",
    "    plt.legend([\"precision\", \"recall\", \"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_through_CV(model, parameters_CV):\n",
    "    # Fit model\n",
    "    model_CV = GridSearchCV(model, parameters_CV, cv=3, n_jobs=5, scoring='neg_log_loss')\n",
    "    model_CV.fit(df_ERA_reduced_features_scaled.loc[train_mask, feature_names_reduced], \n",
    "                 df_ERA_reduced_features_scaled.loc[train_mask, \"Foehn\"])\n",
    "    print(model_CV.best_params_)\n",
    "    \n",
    "    # Predict and optimize threshold\n",
    "    best_model = model_CV.best_estimator_\n",
    "    df_ERA_reduced_features_scaled.loc[:, \"prediction_proba\"] = best_model.predict_proba(df_ERA_reduced_features_scaled.loc[:, feature_names_reduced])[:,1]\n",
    "    precisions, recalls, thresholds = precision_recall_curve(df_ERA_reduced_features_scaled.loc[test_mask, \"Foehn\"], \n",
    "                                                             df_ERA_reduced_features_scaled.loc[test_mask, \"prediction_proba\"])\n",
    "    best_threshold_ERA = thresholds[np.argmin(abs(precisions-recalls))]\n",
    "    df_ERA_reduced_features_scaled.loc[:, \"prediction\"] = (df_ERA_reduced_features_scaled.loc[:, \"prediction_proba\"]> best_threshold_ERA).astype(int)\n",
    "    \n",
    "    # Plot model evaluation curves\n",
    "#     f = plt.figure(figsize=(12,5))\n",
    "#     f.add_subplot(121)\n",
    "#     sns.lineplot(precisions, recalls)\n",
    "#     f.add_subplot(122)\n",
    "#     sns.lineplot(np.append(thresholds, 1.0), precisions)\n",
    "#     sns.lineplot(np.append(thresholds, 1.0), recalls)\n",
    "    \n",
    "    # Print model evaluation scores\n",
    "    \n",
    "    df_ERA_reduced_features_scaled.loc[test_mask, \"prediction\"]\n",
    "    print(f\"Best threshold: {best_threshold_ERA}\")\n",
    "    print(f'Precision: {precision_score(df_ERA_reduced_features_scaled.loc[test_mask, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask, \"prediction\"])}')\n",
    "    print(f'Recall: {recall_score(df_ERA_reduced_features_scaled.loc[test_mask, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask, \"prediction\"])}')\n",
    "    print(confusion_matrix(df_ERA_reduced_features_scaled.loc[test_mask, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask, \"prediction\"]))\n",
    "    display(pd.DataFrame({\"feature_name\": feature_names_reduced, \"importance\": best_model.feature_importances_}).sort_values(by=\"importance\", ascending=False).reset_index(drop=True).head(10))\n",
    "    \n",
    "    precision_scores, recall_scores, f1_scores = [], [], []\n",
    "    for month in range(1,12+1):\n",
    "        test_mask_month =  test_mask & (df_ERA_reduced_features_scaled[\"date\"].dt.month == month)\n",
    "        precision = precision_score(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction\"])\n",
    "        recall = recall_score(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction\"])\n",
    "        f1score = f1_score(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction\"])\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1score)\n",
    "\n",
    "    plot_metrics_by_month(precision_scores, recall_scores, f1_scores)\n",
    "    \n",
    "   \n",
    "    return best_model, best_threshold_ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def train_model_for_each_month(model, parameters):\n",
    "    models_retrained_list, thresholds_list = [], []\n",
    "    \n",
    "    precision_scores, recall_scores, f1_scores = [], [], []\n",
    "    \n",
    "    plt.figure(figsize=(16,9))\n",
    "    for month in range(1,12+1):\n",
    "        # Define train and test masks\n",
    "        train_mask_month = train_mask & (df_ERA_reduced_features_scaled[\"date\"].dt.month == month)\n",
    "        test_mask_month =  test_mask & (df_ERA_reduced_features_scaled[\"date\"].dt.month == month)\n",
    "        all_mask_month =  (df_ERA_reduced_features_scaled[\"date\"].dt.month == month)\n",
    "        \n",
    "        # CV\n",
    "#         model.set_params(scale_pos_weight=df_ERA_reduced_features_scaled.loc[train_mask_month, \"Foehn\"].count()/df_ERA_reduced_features_scaled.loc[train_mask_month, \"Foehn\"].sum())\n",
    "#         print(model.get_params())\n",
    "        model_CV = GridSearchCV(model, parameters, cv=[(slice(None), slice(None))], n_jobs=4, scoring=custom_metric_for_month, verbose=1)\n",
    "        model_CV.fit(df_ERA_reduced_features_scaled.loc[train_mask_month, feature_names_reduced], \n",
    "                     df_ERA_reduced_features_scaled.loc[train_mask_month, \"Foehn\"])\n",
    "        print(model_CV.cv_results_[\"mean_test_score\"])\n",
    "        # Get feature importance\n",
    "#         print(dict(zip(feature_names_reduced, model_CV.best_estimator_.feature_importances_)))\n",
    "        \n",
    "        # Optimize threshold\n",
    "        best_model = model_CV.best_estimator_\n",
    "        df_ERA_reduced_features_scaled.loc[all_mask_month, \"prediction_proba\"] = best_model.predict_proba(df_ERA_reduced_features_scaled.loc[all_mask_month, feature_names_reduced])[:,1]\n",
    "        precisions, recalls, thresholds = precision_recall_curve(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], \n",
    "                                                                 df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction_proba\"])\n",
    "        best_threshold_ERA = thresholds[np.argmin(abs(precisions-recalls))]\n",
    "        df_ERA_reduced_features_scaled.loc[all_mask_month, \"prediction\"] = (df_ERA_reduced_features_scaled.loc[all_mask_month, \"prediction_proba\"]>best_threshold_ERA).astype(int)\n",
    "        \n",
    "        # Print model evaluation scores\n",
    "        precision = precision_score(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction\"])\n",
    "        recall = recall_score(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction\"])\n",
    "        f1score = f1_score(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction\"])\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1score)\n",
    "        \n",
    "        print(f\"Best threshold: {best_threshold_ERA}\")\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'Recall: {recall}')\n",
    "        print(confusion_matrix(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction\"]))\n",
    "        display(pd.DataFrame({\"feature_name\": feature_names_reduced, \"importance\": best_model.feature_importances_}).sort_values(by=\"importance\", ascending=False).reset_index(drop=True).head(10))\n",
    "        \n",
    "        # Retrain model on full data\n",
    "        best_model.fit(df_ERA_reduced_features_scaled.loc[all_mask_month, feature_names_reduced], \n",
    "                       df_ERA_reduced_features_scaled.loc[all_mask_month, \"Foehn\"])\n",
    "        models_retrained_list.append(best_model)\n",
    "        thresholds_list.append(best_threshold_ERA)\n",
    "        \n",
    "    plot_metrics_by_month(precision_scores, recall_scores, f1_scores)\n",
    "    return models_retrained_list, thresholds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# from sklearn.metrics import log_loss, make_scorer\n",
    "def custom_loss(predt: np.ndarray, dtrain: xgb.DMatrix, ERA_len, CESM_len, alpha=100000):\n",
    "    y = dtrain.get_label()[:ERA_len]\n",
    "    \n",
    "    pred = sigmoid(predt)\n",
    "    pred_ERA = pred[:ERA_len]\n",
    "    pred_CESM = pred[ERA_len:]\n",
    "    \n",
    "    grad_logloss  = pred_ERA - y\n",
    "    hess_logloss = pred_ERA*(1.0 - pred_ERA)\n",
    "\n",
    "    \n",
    "#     grad = () + 2*alpha/CESM_len*(np.mean(predt) - np.mean(y))\n",
    "    \n",
    "    grad_custom = 2*alpha/CESM_len*pred_CESM*(1-pred_CESM)*(np.mean(pred_CESM)-np.mean(y))\n",
    "    hess_custom = 2*alpha/CESM_len*pred_CESM*(1-pred_CESM)*((1-pred_CESM)*(np.mean(pred_CESM)-np.mean(y)) -\n",
    "                                                            pred_CESM*(np.mean(pred_CESM)-np.mean(y)) +\n",
    "                                                            pred_CESM*(1-pred_CESM)/CESM_len\n",
    "                                                            )\n",
    "    \n",
    "    grad = np.zeros(len(predt))\n",
    "    grad[:ERA_len] = grad_logloss\n",
    "    grad[ERA_len:] = grad_custom\n",
    "    \n",
    "    hess = np.zeros(len(predt))\n",
    "    hess[:ERA_len] = hess_logloss\n",
    "    hess[ERA_len:] = hess_custom\n",
    "    \n",
    "    return grad, hess\n",
    "\n",
    "def custom_metric_for_month(estimator, X, y):\n",
    "    month_mask_ERA_custom_metric = (df_ERA_reduced_features_scaled[\"date\"].dt.month == month)\n",
    "    month_mask_CESMp_custom_metric = (df_CESMp_reduced_features_scaled[\"date\"].dt.month == month)\n",
    "    \n",
    "    prediction_probas_ERA = estimator.predict_proba(X)[:,1]\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y, prediction_probas_ERA)\n",
    "    best_threshold_ERA = thresholds[np.argmin(abs(precisions-recalls))]                                                         \n",
    "    \n",
    "    foehn_truth = df_ERA_reduced_features_scaled.loc[month_mask_ERA_custom_metric, \"Foehn\"]\n",
    "    CESMp_predictions = (estimator.predict_proba(df_CESMp_reduced_features_scaled.loc[month_mask_CESMp_custom_metric, feature_names_reduced]) >best_threshold_ERA).astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return -np.square(np.mean(CESMp_predictions)-np.mean(foehn_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "monthly_models = 1\n",
    "\n",
    "params = {'max_depth': 4, 'learning_rate': 0.1, \"reg_alpha\":10, \"reg_lambda\":10, \"n_jobs\": 30,\"verbosity\":1, \"seed\": 0}\n",
    "\n",
    "models_retrained_list, thresholds_list = [], []\n",
    "precision_scores, recall_scores, f1_scores= [], [], []\n",
    "\n",
    "most_important_features = set()\n",
    "\n",
    "\n",
    "# Train first on all data\n",
    "D_train = xgb.DMatrix(df_ERA_reduced_features_scaled.loc[train_mask, feature_names_reduced],\n",
    "                      label=df_ERA_reduced_features_scaled.loc[train_mask, \"Foehn\"])\n",
    "model_ges = xgb.Booster(params, [D_train])\n",
    "model_ges = xgb.train(params, dtrain = D_train, num_boost_round=20, xgb_model=model_ges)\n",
    "# for _ in range(50):\n",
    "#     pred = model.predict(D_train)\n",
    "#     g, h = custom_loss(pred, D_train, ERA_len=train_mask_month.sum(), CESM_len = month_mask_CESMp.sum())\n",
    "#     model.boost(D_train, g, h)\n",
    "print(\"Done Fitting on whole Dataset\")\n",
    "\n",
    "for month in range(1,12+1):\n",
    "    train_mask_month = train_mask & (df_ERA_reduced_features_scaled[\"date\"].dt.month == month)\n",
    "    month_mask_ERA = (df_ERA_reduced_features_scaled[\"date\"].dt.month == month)\n",
    "    test_mask_month = test_mask & (df_ERA_reduced_features_scaled[\"date\"].dt.month == month)\n",
    "    month_mask_CESMp = (df_CESMp_reduced_features_scaled[\"date\"].dt.month == month)\n",
    "\n",
    "    df_ERA_CESMp = pd.concat([df_ERA_reduced_features_scaled.loc[train_mask_month, feature_names_reduced + [\"Foehn\"]], \n",
    "                              df_CESMp_reduced_features_scaled.loc[month_mask_CESMp, feature_names_reduced]],\n",
    "                            axis=0)\n",
    "    print(len(df_ERA_CESMp))\n",
    "\n",
    "    D_train = xgb.DMatrix(df_ERA_CESMp[feature_names_reduced],\n",
    "                         label=df_ERA_CESMp[\"Foehn\"])\n",
    "#     model = xgb.Booster(params, [D_train])\n",
    "    model = model_ges.copy()\n",
    "    for _ in range(150):\n",
    "        pred = model.predict(D_train)\n",
    "        g, h = custom_loss(pred, D_train, ERA_len=train_mask_month.sum(), CESM_len = month_mask_CESMp.sum())\n",
    "        model.boost(D_train, g, h)\n",
    "\n",
    "    D_test = xgb.DMatrix(df_ERA_reduced_features_scaled.loc[month_mask_ERA, feature_names_reduced])\n",
    "    yhat = sigmoid(model.predict(D_test))\n",
    "    \n",
    "    \n",
    "    df_ERA_reduced_features_scaled.loc[month_mask_ERA, \"prediction_proba\"] = sigmoid(model.predict(D_test))\n",
    "    precisions, recalls, thresholds = precision_recall_curve(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], \n",
    "                                                             df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction_proba\"])\n",
    "    best_threshold_ERA = thresholds[np.argmin(abs(precisions-recalls))]\n",
    "    thresholds_list.append(best_threshold_ERA)\n",
    "    df_ERA_reduced_features_scaled.loc[month_mask_ERA, \"prediction\"] = (df_ERA_reduced_features_scaled.loc[month_mask_ERA, \"prediction_proba\"]>best_threshold_ERA).astype(int)\n",
    "\n",
    "    # Print model evaluation scores\n",
    "    precision = precision_score(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction\"])\n",
    "    recall = recall_score(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction\"])\n",
    "    f1score = f1_score(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction\"])\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1score)\n",
    "\n",
    "    print(f\"Best threshold: {best_threshold_ERA}\")\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(confusion_matrix(df_ERA_reduced_features_scaled.loc[test_mask_month, \"Foehn\"], df_ERA_reduced_features_scaled.loc[test_mask_month, \"prediction\"]))\n",
    "\n",
    "    importances_month = pd.DataFrame.from_dict(model.get_score(importance_type='weight'), orient=\"index\", columns=[\"importance\"]).sort_values(by=\"importance\", ascending=False)\n",
    "    display(importances_month.head(10))\n",
    "    \n",
    "    most_important_features.update(set(importances_month.head(10).index))\n",
    "\n",
    "\n",
    "    models_retrained_list.append(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "sns.scatterplot(MONTH_NAMES, precision_scores, s=100)\n",
    "sns.scatterplot(MONTH_NAMES, recall_scores, s=100)\n",
    "sns.scatterplot(MONTH_NAMES, f1_scores, s=100)\n",
    "plt.legend([\"Precision\", \"Recall\", \"F1-Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on CESMp and CESMf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if monthly_models:\n",
    "    for month in range(1,12+1):\n",
    "        month_mask_CESMp = (df_CESMp_reduced_features_scaled[\"date\"].dt.month == month )\n",
    "        month_mask_CESMf = (df_CESMf_reduced_features_scaled[\"date\"].dt.month == month )\n",
    "        \n",
    "        xgb_CESMp = xgb.DMatrix(df_CESMp_reduced_features_scaled.loc[month_mask_CESMp, feature_names_reduced])\n",
    "        df_CESMp_reduced_features_scaled.loc[month_mask_CESMp, \"prediction_proba\"] = sigmoid(models_retrained_list[month-1].predict(xgb_CESMp))\n",
    "        df_CESMp_reduced_features_scaled.loc[month_mask_CESMp, \"prediction\"] = (df_CESMp_reduced_features_scaled.loc[month_mask_CESMp, \"prediction_proba\"]> thresholds_list[month-1]).astype(int)\n",
    "        \n",
    "        xgb_CESMf = xgb.DMatrix(df_CESMf_reduced_features_scaled.loc[month_mask_CESMf, feature_names_reduced])\n",
    "        df_CESMf_reduced_features_scaled.loc[month_mask_CESMf, \"prediction_proba\"] = sigmoid(models_retrained_list[month-1].predict(xgb_CESMf))\n",
    "        df_CESMf_reduced_features_scaled.loc[month_mask_CESMf, \"prediction\"] = (df_CESMf_reduced_features_scaled.loc[month_mask_CESMf, \"prediction_proba\"]> thresholds_list[month-1]).astype(int)\n",
    "                                                 \n",
    "df_CESMp_reduced_features_scaled[\"dataset\"] = \"CESMp\"\n",
    "df_CESMf_reduced_features_scaled[\"dataset\"] = \"CESMf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concenate all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for actually observed foehn cases \n",
    "df_foehn = df_ERA_reduced_features_scaled.loc[:, [\"date\", \"Foehn\"]]\n",
    "df_foehn[\"dataset\"] = \"observed_foehn\"\n",
    "df_foehn[\"ensemble\"] = \"observed_foehn\"\n",
    "df_foehn.rename({\"Foehn\": \"prediction\"}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ERA_reduced_features_scaled[\"dataset\"] = \"ERA\"\n",
    "df_ERA_reduced_features_scaled[\"ensemble\"] = \"ERA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all observed foehn, ERAI, CESMp and CESMf prediction\n",
    "df_foehn_ERA_CESMp_CESMf = pd.concat([df_foehn, \n",
    "                                      df_ERA_reduced_features_scaled[[\"date\", \"prediction\", \"dataset\", \"ensemble\"]], \n",
    "                                      df_CESMp_reduced_features_scaled[[\"date\", \"prediction\", \"dataset\", \"ensemble\"]], \n",
    "                                      df_CESMf_reduced_features_scaled[[\"date\", \"prediction\", \"dataset\", \"ensemble\"]]], \n",
    "                                     axis=0, \n",
    "                                     ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might be necessary to transform type of date column (should not)\n",
    "# df_foehn_ERA_CESMp_CESMf[\"date\"] = pd.to_datetime(df_foehn_ERA_CESMp_CESMf[\"date\"], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create month and year column\n",
    "df_foehn_ERA_CESMp_CESMf[\"month\"] = df_foehn_ERA_CESMp_CESMf[\"date\"].dt.month\n",
    "df_foehn_ERA_CESMp_CESMf[\"year\"] = df_foehn_ERA_CESMp_CESMf[\"date\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group dataframe by dataset, ensemble, year, month, and prediction and calculate mean\n",
    "df_monthly_mean = df_foehn_ERA_CESMp_CESMf[[\"date\", \"year\", \"month\", \"dataset\", \"ensemble\", \"prediction\"]].groupby([\"dataset\", \"ensemble\", \"year\", \"month\"], axis=0, as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the test period (1991 until 2000) and future prediction (2091-2100)\n",
    "test_mask_final_plot = ((1990< df_monthly_mean[\"year\"]) & (df_monthly_mean[\"year\"]<2001)) | (2090< df_monthly_mean[\"year\"])\n",
    "df_monthly_mean_testset = df_monthly_mean[test_mask_final_plot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplot for mean foehn frequencies for all datasets\n",
    "plt.figure(figsize=(16,9))\n",
    "fig = sns.boxplot(x=\"month\", y=\"prediction\", data=df_monthly_mean_testset, hue=\"dataset\", hue_order=[\"observed_foehn\", \"ERA\", \"CESMp\", \"CESMf\"])\n",
    "fig.set_xticklabels(MONTH_NAMES);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "ax = plt.axes()\n",
    "\n",
    "for month_test in range(1, 12+1):\n",
    "    plt.figure(1)\n",
    "    CESM_test_distribution = []\n",
    "    df_CESMp_monthly = df_monthly_mean_testset.loc[(df_monthly_mean_testset[\"dataset\"]==\"CESMp\") & (df_monthly_mean_testset[\"month\"]==month_test),:].reset_index()\n",
    "    for i in range(500):\n",
    "        CESM_test_distribution_year = []\n",
    "        for year in range(1991, 2001):\n",
    "            CESM_test_distribution_year.append(df_CESMp_monthly.loc[df_CESMp_monthly[\"year\"]==year, \"prediction\"].sample(1))\n",
    "        CESM_test_distribution.append(np.mean(CESM_test_distribution_year))\n",
    "#     if month_test == 2:\n",
    "#         plt.errorbar(month_test, np.mean(np.array(CESM_test_distribution)), \n",
    "#                      yerr = np.array([[2.34*np.array(CESM_test_distribution).std(),3*np.array(CESM_test_distribution).std()]]).T, \n",
    "#                      color=\"b\", \n",
    "#                      marker=\"o\",\n",
    "#                      markersize=10, \n",
    "#                      linewidth=3, \n",
    "#                      capsize=10, \n",
    "#                      capthick=3, \n",
    "#                      zorder=1)\n",
    "#         plt.scatter(month_test, (df_sum.loc[(df_sum[\"original_dataset\"]==\"ERA\") & (df_sum[\"month\"]==month_test), \"prediction\"]).mean(), \n",
    "#                     marker=\"x\", \n",
    "#                     color=\"r\", \n",
    "#                     s=110, \n",
    "#                     linewidth=5, \n",
    "#                     zorder=10)\n",
    "#     else:\n",
    "    plt.errorbar(month_test, np.mean(np.array(CESM_test_distribution)), \n",
    "                 yerr = 3*np.array(CESM_test_distribution).std(), \n",
    "                 color=\"b\", \n",
    "                 marker=\"o\",\n",
    "                 markersize=10, \n",
    "                 linewidth=3, \n",
    "                 capsize=10, \n",
    "                 capthick=3, \n",
    "                 zorder=1)\n",
    "    plt.scatter(month_test, (df_monthly_mean_testset.loc[(df_monthly_mean_testset[\"dataset\"]==\"observed_foehn\") & (df_monthly_mean_testset[\"month\"]==month_test), \"prediction\"]).mean(), \n",
    "                marker=\"x\", \n",
    "                color=\"r\", \n",
    "                s=110, \n",
    "                linewidth=5, \n",
    "                zorder=10)\n",
    "\n",
    "    if month_test==4:\n",
    "        plt.figure(figsize=(16,9))\n",
    "        sns.distplot(CESM_test_distribution)\n",
    "        plt.plot(df_monthly_mean_testset.loc[(df_monthly_mean_testset[\"dataset\"]==\"ERA\") & (df_monthly_mean_testset[\"month\"]==month_test), \"prediction\"].mean(),0, \n",
    "                \"ro\", \n",
    "                markersize=10)\n",
    "    \n",
    "    if month_test==1: # To make plot prettier\n",
    "        legend = ax.legend([\"Observed foehn mean 1991-2000\", \"CESM-present sampled 10-year periods\"])\n",
    "        \n",
    "ax.set(xticklabels=MONTH_NAMES, xticks=range(1,12+1))\n",
    "ax.set_xlabel(\"Month\",fontsize=15)\n",
    "ax.set_ylabel(\"Mean monthly foehn frequency\",fontsize=15)\n",
    "ax.set_ylim((0,0.50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month_test in range(1, 12+1):\n",
    "    CESMp_test_distribution = []\n",
    "    CESMf_test_distribution = []\n",
    "    df_CESMp_month = df_monthly_mean_testset.loc[(df_monthly_mean_testset[\"dataset\"]==\"CESMp\") & (df_monthly_mean_testset[\"month\"]==month_test),:].reset_index()\n",
    "    df_CESMf_month = df_monthly_mean_testset.loc[(df_monthly_mean_testset[\"dataset\"]==\"CESMf\") & (df_monthly_mean_testset[\"month\"]==month_test),:].reset_index()\n",
    "    st, p = sci.ks_2samp(df_CESMp_month[\"prediction\"], df_CESMf_month[\"prediction\"])\n",
    "\n",
    "    print(MONTH_NAMES[month_test-1] +\"\\t\" +str(np.round(p,5)) + \"\\t\" + str(p<0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ensemble = df_foehn_ERA_CESMp_CESMf.loc[df_foehn_ERA_CESMp_CESMf[\"dataset\"]==\"CESMp\"].groupby([\"ensemble\", \"year\", \"month\"], as_index=False).mean()\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "ax = sns.boxplot(x=\"month\", y=\"prediction\", data=df_ensemble.loc[df_ensemble[\"month\"]==1,:], hue=\"ensemble\", hue_order=[\"E\"+str(i) for i in range(1,35+1)], color=\"tab:blue\")\n",
    "\n",
    "ax.set(xticks=[])\n",
    "ax.set_xlabel(\"January\",fontsize=15)\n",
    "ax.set_ylabel(\"Mean monthly foehn prediction\",fontsize=15)\n",
    "# sns.set(font_scale=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot weathermaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ERA_raw_temp_adjusted = df_ERA_raw.copy()\n",
    "\n",
    "temp_columns = df_ERA_raw_temp_adjusted.filter(regex=\"T_\").columns\n",
    "df_ERA_raw_temp_adjusted[temp_columns] = df_ERA_raw_temp_adjusted[temp_columns]+ 273.15\n",
    "df_ERA_with_stability = calculate_stability(df = df_ERA_raw_temp_adjusted, lats = LATS_CESM_STRING, lons = LONS_CESM_STRING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'SLP'\n",
    "\n",
    "variable_lvl = {\"SLP\": \"sealevel\",\n",
    "                \"Z\": \"850\",\n",
    "                \"T\": \"850\",\n",
    "                \"DELTAPHI\": \"700\",\n",
    "                \"U\": \"700\",\n",
    "                \"V\": \"700\"}\n",
    "\n",
    "unit = {\"SLP\": \"hPa\",\n",
    "        \"Z\": \"meters\",\n",
    "        \"T\": \"$^\\circ$C\",\n",
    "        \"DELTAPHI\": \"$^\\circ$C\",\n",
    "        \"U\": \"m/s\",\n",
    "        \"V\": \"m/s\"}\n",
    "\n",
    "\n",
    "# Altdorf\n",
    "v_limits = {\"SLP\": [1003, 1018],\n",
    "            \"Z\": [1350, 1520],\n",
    "            \"T\": [17+273.15, 21+273.15],\n",
    "            \"DELTAPHI\": [6,11],\n",
    "            \"U\":[5,15],\n",
    "            \"V\":[5,15]}\n",
    "\n",
    "# Piotta\n",
    "# v_limits = {\"SLP\": [1013, 1025],#[1003, 1018],\n",
    "#             \"Z\": [1460, 1530],#[1350, 1520],\n",
    "#             \"T\": [-1+273.15, 6+273.15],\n",
    "#             \"DELTAPHI\": [5.5, 12.5], #[6,11],\n",
    "#             \"U\":[5,15],\n",
    "#             \"V\":[5,15]}\n",
    "\n",
    "df_importances = generate_coordinates_from_feature_label(most_important_features, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kwargs = {\"variable\": variable,\n",
    "          \"variable_lvl\": variable_lvl[variable],\n",
    "          \"unit\": unit[variable],\n",
    "          \"vmin\": v_limits[variable][0],\n",
    "          \"vmax\": v_limits[variable][1],\n",
    "          \"lats_labels\": LATS_CESM_STRING, \n",
    "          \"lons_labels\": LONS_CESM_STRING, \n",
    "          \"df_importances\": df_importances}\n",
    "\n",
    "\n",
    "month = 4\n",
    "month_mask_ERA = df_ERA_with_stability[\"date\"].dt.month == month\n",
    "plot_mean_foehn_condition_for_one_model(**kwargs, \n",
    "                                        model= \"OBS_SouthFoehn\", \n",
    "                                        df = df_ERA_with_stability.loc[:,:], \n",
    "                                        foehn = df_ERA_with_stability.loc[:, \"Foehn\"], \n",
    "                                        )\n",
    "\n",
    "plot_mean_foehn_condition_for_one_model(**kwargs,\n",
    "                                        model= \"ERA_SouthFoehn\",\n",
    "                                        df = df_ERA_with_stability.loc[:,:], \n",
    "                                        foehn = df_ERA_reduced_features_scaled.loc[:, \"prediction\"], \n",
    "                                        )\n",
    "\n",
    "month_mask_CESMp = df_CESMp[\"date\"].dt.month== month\n",
    "plot_mean_foehn_condition_for_one_model(**kwargs, \n",
    "                                        model= \"CESMp_SouthFoehn\", \n",
    "                                        df = df_CESMp.loc[:,:], \n",
    "                                        foehn = df_CESMp_reduced_features_scaled.loc[:, \"prediction\"], \n",
    "                                        )\n",
    "month_mask_CESMf = df_CESMf[\"date\"].dt.month== month\n",
    "plot_mean_foehn_condition_for_one_model(**kwargs, \n",
    "                                        model= \"CESMf_SouthFoehn\",\n",
    "                                        df = df_CESMf.loc[:,:], \n",
    "                                        foehn = df_CESMf_reduced_features_scaled.loc[:, \"prediction\"], \n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
